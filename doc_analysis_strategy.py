#!/usr/bin/env python3
"""
Multi-stage document analysis strategy for comprehensive legal document processing.
Handles large documents while respecting LLM token limits.
"""

import json
from typing import Dict, List, Any
from dataclasses import dataclass

@dataclass
class DocumentChunk:
    """Represents a chunk of a large document for analysis."""
    chunk_id: str
    content: str
    start_char: int
    end_char: int
    char_count: int
    token_estimate: int
    chunk_type: str  # 'header', 'section', 'content', 'footer'

@dataclass
class DocumentAnalysis:
    """Complete analysis of a document across multiple stages."""
    doc_id: str
    url: str
    title: str
    total_chars: int
    total_tokens: int
    
    # Stage results
    summary: str = ""
    key_points: List[str] = None
    legal_requirements: List[str] = None
    compliance_items: List[str] = None
    entities_mentioned: List[str] = None
    sections: List[Dict] = None
    
    # Processing metadata
    chunks_processed: int = 0
    processing_strategy: str = ""
    confidence_score: float = 0.0

def analyze_document_strategy():
    """Demonstrate multi-stage document analysis strategies."""
    
    print("üìã COMPREHENSIVE DOCUMENT ANALYSIS STRATEGIES")
    print("=" * 60)
    
    strategies = {
        "1. HIERARCHICAL CHUNKING": {
            "description": "Split documents by logical sections",
            "approach": [
                "Extract document structure (headers, sections)",
                "Create chunks by section boundaries", 
                "Analyze each section separately",
                "Synthesize section analyses into full report"
            ],
            "token_usage": "3-5K tokens per chunk",
            "quality": "HIGH - preserves document structure",
            "use_case": "Legal documents with clear sections"
        },
        
        "2. SLIDING WINDOW": {
            "description": "Overlapping chunks with context preservation",
            "approach": [
                "Create 10K char chunks with 2K char overlap",
                "Analyze each chunk with previous context",
                "Track themes and entities across chunks",
                "Build comprehensive analysis incrementally"
            ],
            "token_usage": "3K tokens per chunk",
            "quality": "HIGH - maintains context continuity", 
            "use_case": "Dense legal texts without clear structure"
        },
        
        "3. EXTRACTIVE + ABSTRACTIVE": {
            "description": "Extract key info first, then analyze",
            "approach": [
                "Stage 1: Extract entities, dates, requirements (fast)",
                "Stage 2: Analyze extracted content (focused)",
                "Stage 3: Generate comprehensive report",
                "Stage 4: Cross-reference with full text"
            ],
            "token_usage": "1-2K per extraction, 5K for analysis",
            "quality": "MEDIUM-HIGH - efficient but may miss nuances",
            "use_case": "Large document sets needing quick processing"
        },
        
        "4. SUMMARIZE + DRILL-DOWN": {
            "description": "Progressive detail levels",
            "approach": [
                "Level 1: Generate executive summary (500 chars)",
                "Level 2: Detailed summary (2K chars)", 
                "Level 3: Section-by-section analysis",
                "Level 4: Deep dive on critical sections"
            ],
            "token_usage": "Scales from 500 to 10K tokens",
            "quality": "ADAPTIVE - matches analysis depth to importance",
            "use_case": "Mixed document types with varying importance"
        },
        
        "5. PARALLEL PROCESSING": {
            "description": "Multiple specialized analyses simultaneously",
            "approach": [
                "Chunk 1: Legal requirements extraction",
                "Chunk 2: Compliance obligations analysis", 
                "Chunk 3: Entity and relationship mapping",
                "Chunk 4: Risk and impact assessment",
                "Final: Synthesize all analyses"
            ],
            "token_usage": "2-3K per specialized analysis",
            "quality": "HIGH - comprehensive multi-angle analysis",
            "use_case": "Complex legal documents needing thorough review"
        }
    }
    
    for strategy_name, details in strategies.items():
        print(f"\n{strategy_name}")
        print("-" * 40)
        print(f"üìù {details['description']}")
        print(f"üîÑ Approach:")
        for step in details['approach']:
            print(f"   ‚Ä¢ {step}")
        print(f"üéØ Token usage: {details['token_usage']}")
        print(f"‚≠ê Quality: {details['quality']}")
        print(f"üíº Best for: {details['use_case']}")
    
    return strategies

def design_optimal_strategy(doc_sizes: List[int], analysis_requirements: List[str]) -> Dict:
    """Design optimal strategy based on document characteristics and requirements."""
    
    print(f"\nüéØ STRATEGY RECOMMENDATION ENGINE")
    print("=" * 40)
    
    # Analyze document characteristics
    total_docs = len(doc_sizes)
    avg_size = sum(doc_sizes) // max(total_docs, 1)
    max_size = max(doc_sizes) if doc_sizes else 0
    large_docs = sum(1 for size in doc_sizes if size > 100000)
    
    print(f"üìä Document Profile:")
    print(f"   Total documents: {total_docs}")
    print(f"   Average size: {avg_size:,} chars")
    print(f"   Largest document: {max_size:,} chars")
    print(f"   Large docs (>100K): {large_docs}")
    
    # Requirements analysis
    print(f"\nüìã Analysis Requirements:")
    for req in analysis_requirements:
        print(f"   ‚Ä¢ {req}")
    
    # Strategy selection logic
    if max_size > 1000000:  # Very large docs (>1MB)
        strategy = "HIERARCHICAL CHUNKING + PARALLEL PROCESSING"
        reasoning = "Large documents need structured approach with specialized analysis"
        token_budget = "50-100K tokens total"
        
    elif large_docs > total_docs // 2:  # Mostly large docs
        strategy = "SUMMARIZE + DRILL-DOWN"
        reasoning = "Mixed sizes benefit from adaptive depth analysis"
        token_budget = "30-60K tokens total"
        
    elif "comprehensive analysis" in str(analysis_requirements).lower():
        strategy = "PARALLEL PROCESSING"
        reasoning = "Comprehensive requirements need multi-angle analysis"
        token_budget = "40-80K tokens total"
        
    elif "quick overview" in str(analysis_requirements).lower():
        strategy = "EXTRACTIVE + ABSTRACTIVE"
        reasoning = "Quick processing needs efficient extraction first"
        token_budget = "15-30K tokens total"
        
    else:
        strategy = "SLIDING WINDOW"
        reasoning = "Balanced approach for general document analysis"
        token_budget = "25-50K tokens total"
    
    print(f"\nüéØ RECOMMENDED STRATEGY: {strategy}")
    print(f"üí≠ Reasoning: {reasoning}")
    print(f"üé´ Token budget: {token_budget}")
    
    return {
        "strategy": strategy,
        "reasoning": reasoning,
        "token_budget": token_budget,
        "doc_profile": {
            "total_docs": total_docs,
            "avg_size": avg_size,
            "max_size": max_size,
            "large_docs": large_docs
        }
    }

def create_implementation_plan(strategy_name: str, doc_sizes: List[int]) -> Dict:
    """Create detailed implementation plan for the chosen strategy."""
    
    print(f"\nüîß IMPLEMENTATION PLAN: {strategy_name}")
    print("=" * 50)
    
    if "HIERARCHICAL" in strategy_name:
        plan = {
            "approach": "Document Structure Analysis",
            "steps": [
                {
                    "stage": "1. Structure Detection",
                    "description": "Extract document sections and headers",
                    "function": "extract_document_structure()",
                    "token_usage": "~1K per document",
                    "output": "Section map with boundaries"
                },
                {
                    "stage": "2. Section Analysis", 
                    "description": "Analyze each section separately",
                    "function": "analyze_section(section_content, context)",
                    "token_usage": "~3K per section",
                    "output": "Section-specific insights"
                },
                {
                    "stage": "3. Cross-Section Synthesis",
                    "description": "Connect insights across sections",
                    "function": "synthesize_sections(section_analyses)",
                    "token_usage": "~5K for synthesis",
                    "output": "Comprehensive document analysis"
                },
                {
                    "stage": "4. Report Generation",
                    "description": "Generate final structured report",
                    "function": "generate_comprehensive_report()",
                    "token_usage": "~3K for report",
                    "output": "Executive summary + detailed findings"
                }
            ],
            "total_token_estimate": f"~{len(doc_sizes) * 12}K tokens",
            "processing_time": "Sequential: ~2-3 min per doc",
            "quality_score": "95% - Preserves document structure"
        }
        
    elif "PARALLEL" in strategy_name:
        plan = {
            "approach": "Multi-Aspect Simultaneous Analysis",
            "steps": [
                {
                    "stage": "1. Content Preparation",
                    "description": "Chunk documents optimally for analysis",
                    "function": "prepare_analysis_chunks()",
                    "token_usage": "~500 per document",
                    "output": "Analysis-ready chunks"
                },
                {
                    "stage": "2A. Legal Requirements",
                    "description": "Extract legal obligations and requirements",
                    "function": "extract_legal_requirements(chunks)",
                    "token_usage": "~3K per document",
                    "output": "Structured legal requirements"
                },
                {
                    "stage": "2B. Compliance Analysis",
                    "description": "Identify compliance obligations",
                    "function": "analyze_compliance_obligations(chunks)",
                    "token_usage": "~3K per document", 
                    "output": "Compliance checklist"
                },
                {
                    "stage": "2C. Entity Extraction",
                    "description": "Extract entities and relationships",
                    "function": "extract_entities_and_relationships(chunks)",
                    "token_usage": "~2K per document",
                    "output": "Entity relationship map"
                },
                {
                    "stage": "3. Synthesis & Report",
                    "description": "Combine all analyses into comprehensive report",
                    "function": "synthesize_multi_aspect_analysis()",
                    "token_usage": "~5K for synthesis",
                    "output": "Multi-dimensional analysis report"
                }
            ],
            "total_token_estimate": f"~{len(doc_sizes) * 13}K tokens",
            "processing_time": "Parallel: ~1-2 min per doc",
            "quality_score": "98% - Comprehensive multi-angle analysis"
        }
        
    else:  # SLIDING WINDOW default
        plan = {
            "approach": "Overlapping Context Windows",
            "steps": [
                {
                    "stage": "1. Window Planning",
                    "description": "Plan optimal window sizes and overlaps",
                    "function": "plan_sliding_windows()",
                    "token_usage": "~100 per document",
                    "output": "Window boundaries with overlap zones"
                },
                {
                    "stage": "2. Window Analysis",
                    "description": "Analyze each window with context",
                    "function": "analyze_window(window, previous_context)",
                    "token_usage": "~3K per window",
                    "output": "Window-specific insights with continuity"
                },
                {
                    "stage": "3. Context Threading",
                    "description": "Thread insights across windows",
                    "function": "thread_insights_across_windows()",
                    "token_usage": "~2K per document",
                    "output": "Continuous narrative analysis"
                },
                {
                    "stage": "4. Final Integration",
                    "description": "Integrate all windows into complete analysis",
                    "function": "integrate_windowed_analysis()",
                    "token_usage": "~4K per document",
                    "output": "Complete document analysis"
                }
            ],
            "total_token_estimate": f"~{sum(doc_sizes) // 3000 * 3 + len(doc_sizes) * 6}K tokens",
            "processing_time": "Sequential: ~1.5-2 min per doc",
            "quality_score": "90% - Good context preservation"
        }
    
    # Print the plan
    for step in plan["steps"]:
        print(f"\n{step['stage']}")
        print(f"   üìù {step['description']}")
        print(f"   üîß Function: {step['function']}")
        print(f"   üé´ Token usage: {step['token_usage']}")
        print(f"   üì§ Output: {step['output']}")
    
    print(f"\nüìä PLAN SUMMARY:")
    print(f"   Total token estimate: {plan['total_token_estimate']}")
    print(f"   Processing time: {plan['processing_time']}")
    print(f"   Quality score: {plan['quality_score']}")
    
    return plan

def main():
    """Demonstrate document analysis strategy selection."""
    
    print("üìÑ COMPREHENSIVE DOCUMENT ANALYSIS STRATEGY GUIDE")
    print("Solving the challenge of analyzing large legal documents")
    print("=" * 70)
    
    # Step 1: Show available strategies
    strategies = analyze_document_strategy()
    
    # Step 2: Analyze your actual document profile
    # Based on your test results
    actual_doc_sizes = [27730, 1534, 322191, 5734, 27730, 31075, 1650339, 1975369, 2154456]
    analysis_requirements = [
        "Legal compliance analysis",
        "Extract regulatory requirements", 
        "Identify environmental obligations",
        "Generate compliance reports",
        "Cross-reference with project requirements"
    ]
    
    recommendation = design_optimal_strategy(actual_doc_sizes, analysis_requirements)
    
    # Step 3: Create implementation plan
    implementation = create_implementation_plan(recommendation["strategy"], actual_doc_sizes)
    
    print(f"\nüéØ NEXT STEPS:")
    print(f"1. Implement the {recommendation['strategy']} approach")
    print(f"2. Budget {recommendation['token_budget']} for processing")
    print(f"3. Process documents in stages as outlined above")
    print(f"4. Generate comprehensive reports with full document context")
    
    print(f"\nüí° KEY BENEFITS:")
    print(f"   ‚úÖ Analyze complete documents without token overflow")
    print(f"   ‚úÖ Maintain document context and structure")
    print(f"   ‚úÖ Generate comprehensive compliance reports")
    print(f"   ‚úÖ Handle documents from 1KB to 2MB+ in size")
    print(f"   ‚úÖ Scalable to hundreds of documents")

if __name__ == "__main__":
    main()

